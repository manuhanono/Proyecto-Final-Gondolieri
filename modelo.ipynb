{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de LightGlue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'LightGlue' already exists and is not an empty directory.\n",
      "/Users/manu/Proyecto Final/Proyecto-Final-Gondolieri/LightGlue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manu/Library/Python/3.9/lib/python/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "## En teoria ya esta en el repo, pero por si acaso\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "if Path.cwd().name != \"LightGlue\":\n",
    "    !git clone --quiet https://github.com/cvg/LightGlue/\n",
    "    %cd LightGlue\n",
    "    !pip install --progress-bar off --quiet -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26.4 in /Users/manu/opt/anaconda3/lib/python3.9/site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "## Importo las librerias necesarias\n",
    "! pip install numpy==1.26.4\n",
    "import torch\n",
    "from LightGlue.lightglue import LightGlue, SuperPoint, DISK, SIFT, ALIKED, DoGHardNet\n",
    "from LightGlue.lightglue.utils import load_image, rbd\n",
    "from LightGlue.lightglue import viz2d\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino parámetros de configuración\n",
    "min_matches = 25  # Número mínimo de correspondencias necesarias para considerar que el objeto está presente\n",
    "margin = 50  # Margen en píxeles alrededor del área recortada\n",
    "overlap = 120  # Margen de superposición entre las partes\n",
    "\n",
    "# Configurar el dispositivo y los modelos\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "extractor = DoGHardNet(max_num_keypoints=2048).eval().to(device) #es donde se puede llamar a los modelos: DogHardNet, SuperPoint, SIFT, ALIKED, DISK\n",
    "matcher = LightGlue(features=\"doghardnet\").eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analizando el producto: TRESEMME SH LISO BOTOX ST P 12X500ML - 7791293050119.png\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Procesar cada partición de la imagen de la góndola\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, gondola_split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m([gondola_upper, gondola_lower]):\n\u001b[0;32m---> 28\u001b[0m     feats0 \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproduct_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     feats1 \u001b[38;5;241m=\u001b[39m extractor\u001b[38;5;241m.\u001b[39mextract(gondola_split\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     30\u001b[0m     matches01 \u001b[38;5;241m=\u001b[39m matcher({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage0\u001b[39m\u001b[38;5;124m\"\u001b[39m: feats0, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage1\u001b[39m\u001b[38;5;124m\"\u001b[39m: feats1})\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Proyecto Final/Proyecto-Final-Gondolieri/LightGlue/lightglue/utils.py:144\u001b[0m, in \u001b[0;36mExtractor.extract\u001b[0;34m(self, img, **conf)\u001b[0m\n\u001b[1;32m    142\u001b[0m shape \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    143\u001b[0m img, scales \u001b[38;5;241m=\u001b[39m ImagePreprocessor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_conf, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconf})(img)\n\u001b[0;32m--> 144\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m feats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(shape)[\u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mto(img)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    146\u001b[0m feats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (feats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m/\u001b[39m scales[\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n",
      "File \u001b[0;32m~/Proyecto Final/Proyecto-Final-Gondolieri/LightGlue/lightglue/dog_hardnet.py:32\u001b[0m, in \u001b[0;36mDoGHardNet.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     30\u001b[0m     w, h \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_size\u001b[39m\u001b[38;5;124m\"\u001b[39m][k]\n\u001b[1;32m     31\u001b[0m     img \u001b[38;5;241m=\u001b[39m img[:, : h\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint32), : w\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint32)]\n\u001b[0;32m---> 32\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_single_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m lafs \u001b[38;5;241m=\u001b[39m laf_from_center_scale_ori(\n\u001b[1;32m     34\u001b[0m     p[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;241m6.0\u001b[39m \u001b[38;5;241m*\u001b[39m p[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscales\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     36\u001b[0m     torch\u001b[38;5;241m.\u001b[39mrad2deg(p[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moris\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     37\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     38\u001b[0m p[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescriptors\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlaf_desc(img[\u001b[38;5;28;01mNone\u001b[39;00m], lafs)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m128\u001b[39m)\n",
      "File \u001b[0;32m~/Proyecto Final/Proyecto-Final-Gondolieri/LightGlue/lightglue/sift.py:141\u001b[0m, in \u001b[0;36mSIFT.extract_single_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_single_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 141\u001b[0m     image_np \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpycolmap\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(pycolmap\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.5.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "# Función para dividir la imagen de la góndola en 2 partes horizontales con superposición\n",
    "def split_image_horizontal_with_overlap(image, overlap):\n",
    "    h, w = image.shape[1], image.shape[2]\n",
    "    split_height = h // 2\n",
    "\n",
    "    upper_split = image[:, :split_height + overlap, :]\n",
    "    lower_split = image[:, max(0, split_height - overlap):, :]\n",
    "\n",
    "    return upper_split, lower_split\n",
    "\n",
    "# Cargar la imagen de la góndola\n",
    "gondola_image_path = \"/Users/manu/Downloads/Mayo-Junio-Julio (teoricamente) 2/39-MJJ-.jpeg\"\n",
    "gondola_image = load_image(gondola_image_path)\n",
    "\n",
    "gondola_upper, gondola_lower = split_image_horizontal_with_overlap(gondola_image, overlap)\n",
    "\n",
    "# Ruta a la carpeta de productos\n",
    "product_folder = Path(\"/Users/manu/Downloads/Cuidado Capilar/Tresseme\")\n",
    "\n",
    "# Procesar cada imagen de producto\n",
    "for product_image_path in product_folder.glob('*'):  \n",
    "    if product_image_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.avif', '.webp']: \n",
    "        print(f\"Analizando el producto: {product_image_path.name}\")\n",
    "\n",
    "        product_image = load_image(product_image_path)\n",
    "        # Procesar cada partición de la imagen de la góndola\n",
    "        for idx, gondola_split in enumerate([gondola_upper, gondola_lower]):\n",
    "            feats0 = extractor.extract(product_image.to(device))\n",
    "            feats1 = extractor.extract(gondola_split.to(device))\n",
    "            matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n",
    "            feats0, feats1, matches01 = [\n",
    "                rbd(x) for x in [feats0, feats1, matches01]\n",
    "            ] \n",
    "\n",
    "            kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n",
    "            m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n",
    "\n",
    "            # Visualizar los resultados\n",
    "            axes = viz2d.plot_images([product_image, gondola_split])\n",
    "            viz2d.plot_matches(m_kpts0, m_kpts1, color=\"lime\", lw=0.2)\n",
    "            viz2d.add_text(0, f'Stop after {matches01[\"stop\"]} layers in split {idx}', fs=20)\n",
    "\n",
    "\n",
    "            # Verificar si el número de correspondencias es suficiente\n",
    "            if len(matches) >= min_matches:\n",
    "                print(f\"Objeto detectado con {len(matches)} correspondencias en la partición {idx} para el producto: {product_image_path.name}\")\n",
    "\n",
    "                # Cargar las bounding boxes del archivo JSON\n",
    "                json_path = gondola_image_path.replace(Path(gondola_image_path).suffix, '.json')\n",
    "                with open(json_path, 'r') as f:\n",
    "                    bounding_boxes = json.load(f)\n",
    "\n",
    "                # Verificar si el producto tiene una bounding box en el JSON\n",
    "                product_name = product_image_path.stem\n",
    "                if product_name in bounding_boxes:\n",
    "                    bbox = bounding_boxes[product_name]\n",
    "                    x_min, y_min, x_max, y_max = bbox\n",
    "\n",
    "                    # Contar cuántos puntos de coincidencia están dentro de la bounding box\n",
    "                    in_bbox_count = sum(\n",
    "                        1 for x, y in m_kpts1 if x_min <= x <= x_max and y_min <= y <= y_max\n",
    "                    )\n",
    "\n",
    "                    # Verificar si la mayoría de los puntos de coincidencia están dentro de la bounding box\n",
    "                    if in_bbox_count / len(m_kpts1) > 0.5:\n",
    "                        print(f\"Producto {product_name} detectado correctamente en la partición {idx} con {in_bbox_count} puntos dentro de la bounding box.\")\n",
    "                    else:\n",
    "                        print(f\"Falso positivo para el producto {product_name} en la partición {idx}. Solo {in_bbox_count} puntos dentro de la bounding box.\")\n",
    "                else:\n",
    "                    print(f\"Falso positivo para el producto {product_name} en la partición {idx}. No se encontró bounding box en el JSON.\")\n",
    "            else:\n",
    "                print(f\"Objeto no detectado o no confiable, solo {len(matches)} correspondencias encontradas en la partición {idx} para el producto: {product_image_path.name}\")\n",
    "\n",
    "        print(f\"Análisis completado para el producto: {product_image_path.name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
